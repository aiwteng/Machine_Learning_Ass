# -*- coding: utf-8 -*-
"""ML_Assinmgnet (Group Doremi).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kWIKU4K6S8NiHXF99bxjMkiJrg0EsFzc

# Kidney Stone Prediction

### Files From The Dataset
- **train.csv** - the training dataset; target is the likelihood of a kidney stone being present
- **test.csv** - the test dataset; your objective is to predict the probability of target

### About the Data
The six physical characteristics of the urine are:

**`(1) specific gravity`**

The density of the urine relative to water. The normal range for urine specific gravity is 1.005 to 1.030. USG values vary between 1.000 and 1.040 g/mL, USG less than 1.008 g/mL is regarded as dilute, and USG greater than 1.020 g/mL is considered concentrated.


**`(2) pH`**

The negative logarithm of the hydrogen ion. When the pH of urine drops below 5.5, urine becomes saturated with uric acid crystals, a condition known as hypercalciuria. When there is too much uric acid in the urine, stones can form. Uric acid stones are more common in people who consume large amounts of protein, such as that found in red meat or poultry.

**`(3) osmolarity (mOsm)`**

A unit used in biology and medicine but not in physical chemistry. Osmolarity is proportional to the concentration of molecules in solution.

**`(4) conductivity (mMho)`**

One Mho is one reciprocal Ohm. Conductivity is proportional to the concentration of charged ions in solution.

**`(5) urea concentration (mmol/L)`**

The concentration of urea in the urine, measured in millimoles per litre.

**`(6) calcium concentration (mmol/L)`**

The concentration of calcium in the urine, measured in millimoles per litre.

## Loading Dataset
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

import warnings
warnings.filterwarnings('ignore')

"""## Data Analysis"""

train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

train=train.drop(columns=['id'])
# test = test.drop('id',axis=1)
# train = train.drop('target',axis=1)

# Create a list variable
column=['gravity', 'ph', 'osmo', 'cond', 'urea', 'calc']

train.head()

test.head()

train.shape

test.shape

train.info()

test.info()

train.describe()

test.describe()

"""### Checking duplicated rows and columns"""

duplicatedrows = train.duplicated().sum()
print("Number of duplicate rows in train dataset: ", duplicatedrows)

# Transpose to find duplicated column
duplicatedcols = train.T.duplicated().sum()
print("Number of duplicate columns in train dataset: ", duplicatedcols)

# Remove duplicated values
train.drop_duplicates(inplace=True)

"""### Checking missing values"""

# finding missing values
for col in train.columns:
    null_rate = train[col].isnull().sum() / len(train)
    print("%s: %s"%(col, null_rate))

"""### Correlation Heatmap

The correlation heatmaps show the correlation coefficients between different variables in the dataset. The heatmaps are color-coded to indicate the strength and direction of the correlation. A positive correlation is shown in shades of red, while a negative correlation is shown in shades of blue.
"""

plt.subplots(figsize=(15,15))
ax = plt.axes()
ax.set_title("Correlation Heatmap")
corr = train.corr() # core code
sns.heatmap(corr,
            annot = True,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)

plt.subplots(figsize=(15,15))
ax = plt.axes()
ax.set_title("Correlation Heatmap")
corr = test.corr() # core code
sns.heatmap(corr,
            annot = True,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)

"""### Pair plots

Pair Plots is use to identify patterns and trends in the dataset. They can also help us to identify the relationship between the features (variables) of the dataset.

 From the output below, it displayed output allows us to analyze the variability present in each plot. The plots are arranged in a matrix format where the row names correspond to the x-axis and the column names correspond to the y-axis. The subplots on the main diagonal represent the univariate histograms or distributions of each attribute.
"""

import seaborn as sns
import matplotlib.pyplot as plt

# Set style and size for plot
sns.set(style = "ticks", font_scale=1.2)
plt.figure(figsize = (12,10))

# Create pairplot
sns.pairplot(train, hue="target")

# Set plot title and legend
plt.title("Insights in Data")
plt.legend(title = " Target", loc="upper right")

# Adjust plot layout and show plot
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,6))
plt.title("Distribution of feature Data in the Train Dataset")

for i, column in enumerate(train.columns,1):
    plt.subplot(4,3,i)
    plt.title(f"Distribution of {column} Data")
    sns.histplot(train[column], kde = True)
    plt.xlabel(column)
    plt.tight_layout()
    plt.plot()

plt.show()

"""- Distribution of  `pH`, `urea`, `calc` are skewed positively
- Distribution of `gravity`,`cond`, `osmo` are skewed negatively

### BoxPlot

Box plots are used to display the distribution of data and to identify potential outliers in the data.

Outlier is an observation that is significantly different from other obserations in a dataset

From below output, we can see that there are outliers for gravity and pH value.
"""

# Set figure size and layout
fig, axs = plt.subplots(ncols=3, nrows=2, figsize = (10,4))
axs = axs.flatten()

# Loop through each feature and plot its boxplot with outliers highlited

for i, column in enumerate(train.columns[:-1]):
    sns.boxplot(x=train[column],ax=axs[i], color='lightgreen')
    sns.stripplot(x=train[column][train[column]>train[column].quantile(0.75)],ax=axs[i],color='red',size=4)
    axs[i].set_title(column)

# Remove extra subplot(s) if any
for ax in axs[len(train.columns):-1]:
        ax.remove()

#Adjust spacing between subplots
fig.tight_layout()

# Show the plot
plt.show()

# Save the plot as a PNG file
fig.savefig('boxplot1.png', dpi=300, bbox_inches= 'tight')

"""### Violin Plots

Violin plot can reveal information about the shape, spread, and skewness of data, as well as any outliers of gaps in the data.
"""

# Set figure size and layout
fig, axs = plt.subplots(ncols = 3, nrows=2, figsize=(12,8))
axs = axs.flatten()

# Loop through each feature and plot its violin plot
for i, column in enumerate(train.columns[:-1]):
    sns.violinplot(y=train[column], ax = axs[i],color='pink')
    axs[i].set_title(column)

#Remove extra subplot(s) if any
for ax in axs[len(train.columns)-1:]:
    ax.remove()

#Adjust spacing between subplots
fig.tight_layout()

# Show the plot
plt.show()

"""` Gravity `
- The median value is around 1.018
- It has relatively narrow distribution
- First Quartile is 1.011
- Third Quartile is 1.022
- Interquatile range is 0.011

` pH `
- The median value is around 5.55
- It has wider distribution than gravity
- First Quartile is 5.5
- Third Quartile is 6.05
- Interquatile range is 0.55

` osmo `
- The median value is around 660
- It has a wide distribution
- First Quartile is 440
- Third Quartile is 840
- Interquatile range is 400

` cond `
- The median value is around 22
- It has a relatively narrow distribution
- First Quartile is 15
- Third Quartile is 28
- Interquatile range is 13

` urea `
- The median value is around 270
- It has a wide distribution
- First Quartile is 170
- Third Quartile is 390
- Interquatile range is 220

` calc `
- The median value is around 22
- It has a wide distribution
- First Quartile is 1.5
- Third Quartile is 6.5
- Interquatile range is 5.0

## Data preprocessing

### Removing outliers using winsorization
"""

import numpy as np
from scipy.stats.mstats import winsorize

# Winsorize 'gravity' variable
winsorized_gravity = winsorize(train['gravity'], (0.05, 0.05))
train['gravity'] = winsorized_gravity

# Winsorize 'ph' variable
winsorized_ph = winsorize(train['ph'], (0.05, 0.05))
train['ph'] = winsorized_ph

fig, axs = plt.subplots(ncols=3, nrows=2, figsize=(10, 4))
axs = axs.flatten()

# Loop through each feature and plot its boxplot with outliers highlighted
for i, col in enumerate(train.columns[:-1]):
    sns.boxplot(x=train[col], ax=axs[i], color='lightgreen')
    sns.stripplot(x=train[col][train[col] > train[col].quantile(0.75)], ax=axs[i], color='red', size=4)
    axs[i].set_title(col)

# Remove extra subplot(s) if any
for ax in axs[len(train.columns)-1:]:
    ax.remove()

# Adjust spacing between subplots
fig.tight_layout()

# Show the plot
plt.show()

# Save the plot as a PNG file
fig.savefig('boxplot2.png', dpi=300, bbox_inches='tight')

"""### Train-Test Split"""

# Separate the features and target variable
X = train.drop('target', axis=1)
y = train['target']

# Split the train_data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# print(X_test)

# Preprocess the numerical features by scaling them and adding polynomial features
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# print(X_test)

print('\n','\033[1mStandardardization on Training set'.center(120))
X_train = pd.DataFrame(X_train, columns = X.columns)
X_train.head()

print('\n','\033[1mStandardardization on Testing set'.center(120))
X_test = pd.DataFrame(X_test, columns = X.columns)
X_test.head()

"""#### Scaling the, and adding polynomial features"""

# Preprocess the numerical features by scaling them and adding polynomial features
from sklearn.preprocessing import StandardScaler, PolynomialFeatures

poly = PolynomialFeatures(degree=2)
X_train = scaler.fit_transform(X_train)
# X_test = scaler.transform(X_test)


print('\n','\033[1mStandardardization on Training set'.center(120))
X_train = pd.DataFrame(X_train, columns = X.columns)
# X_train.head()
y_test.describe()

"""## Model Building
### Random Forest
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
import pandas as pd
import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

#Create an instance of the RandomForest classifier with regularization parameter
rf = RandomForestClassifier(n_estimators=100, max_depth=10,min_samples_split=5,min_samples_leaf=2,class_weight='balanced', random_state=42)

#Train the model
rf.fit(X_train, y_train)

# Evaluate the model
y_pred_rf = rf.predict(X_test)
cm = confusion_matrix(y_test,y_pred_rf)
accuracy_rf = accuracy_score(y_test, y_pred_rf)*100
f1 = f1_score(y_test,y_pred_rf)
classification_rep = classification_report(y_test, y_pred_rf)


# Make predictions on the test dataset
pd.set_option('display.max_rows', None) # To show all data
predictions = rf.predict(X_test)
results_rf = pd.DataFrame({'id':y_test.index,'target':predictions})

#Print the evaluation matrix
print("Confusion Matrix: ", cm)
print("Classification Report:")
print(classification_rep)
print('F1 score:',f1)
print("Accuracy: " , accuracy_rf)
print()

# Hyperparameter Tuning for Random Forest
from sklearn.model_selection import GridSearchCV

# Define the parameter grid for hyperparameter tuning
param_grid = {
    'n_estimators':[100,200,300],
    'max_depth': [None,5,10],
    'min_samples_split': [2,5,10]
}

# Create an instance of the RandomForest classifier
rf = RandomForestClassifier(random_state=2023)

# Create a GridSearchCV object and fit it to the training data
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid,cv=5)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters and the corresponding model
best_params = grid_search.best_params_
best_model_rf = grid_search.best_estimator_

# print the best hyperparameters
print("Best Hyperparameters For Random Forest: ")
print(best_params)

from sklearn.metrics import roc_auc_score, confusion_matrix,classification_report

#Train the model with the best hyperparameters
best_model_rf.fit(X_train, y_train)

# Predict probabilities for the validation set
y_pred_prob = best_model_rf.predict_proba(X_test)[:,1]
roc_auc_rf = roc_auc_score(y_test,y_pred_prob)

#Evaluate the model on the test data
y_pred = best_model_rf.predict(X_test)
cm = confusion_matrix(y_test,y_pred)
f1 = f1_score(y_test,y_pred)
cr_rf= classification_report(y_test, y_pred)
accuracy_rf_best = accuracy_score(y_test, y_pred)*100

#Print the evaluation matrix
print('Random Forest after Hyperparameter Tuning:')
print("Confusion Matrix: \n", cm)
print("Classification Report:\n ",cr_rf)
print("Accuracy: ", accuracy_rf_best)
print('F1 Score:',f1)
print("ROC AUC Score:", roc_auc_rf)
print()

#Make predictions on the test dataset
pd.set_option('display.max_rows', None)
predictions = best_model_rf.predict(X_test)
results_rf = pd.DataFrame({'id':y_test.index,'target': predictions})

from sklearn.model_selection import learning_curve

# Define the Random Forest model (already trained with the best hyperparameters)
model_rf = best_model_rf

# Generate the learning curve data
train_sizes, train_scores, test_scores = learning_curve(
    model_rf, X_train, y_train, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10))

# Calculate the mean and standard deviation of training and test scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, label='Training Accuracy')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.3)
plt.plot(train_sizes, test_mean, label='Validation Accuracy')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.3)
plt.title('Learning Curve - Random Forest')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

"""### Logistic regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score, roc_auc_score
import pandas as pd
import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

# Create an instance of the Logistic Regression classifier
logreg = LogisticRegression(random_state=42)

# Train the model
logreg.fit(X_train, y_train)

# Evaluate the model
y_pred_logreg = logreg.predict(X_test)
cm = confusion_matrix(y_test, y_pred_logreg)
accuracy_logreg = accuracy_score(y_test, y_pred_logreg) * 100
f1 = f1_score(y_test, y_pred_logreg)
classification_rep = classification_report(y_test, y_pred_logreg)

# Print the evaluation matrix
print("Confusion Matrix: ", cm)
print("Classification Report:")
print(classification_rep)
print('F1 score:', f1)
print("Accuracy: ", accuracy_logreg)
print()

# Hyperparameter Tuning for Logistic Regression
from sklearn.model_selection import GridSearchCV

#Define the hyperparameter grid
param_grid={
    'C':[0.001, 0.01, 0.1,1,10],
    'penalty': ['l1','l2','l3'],
    'solver': ['liblinear','newton-cholesky', 'newton-cg', 'sag', 'saga']
}

# #Initialize the logistic regression model
# logreg = LogisticRegression()

# Create the grid search object
grid_search_lr = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5, scoring='accuracy')

# Fit the grid search object
grid_search_lr.fit(X_train, y_train)

# Get the best hyperparameters and mode;
best_params = grid_search_lr.best_params_
best_model_logreg = grid_search_lr.best_estimator_

#Print the best hyperparameters
print('Best Hyperparameters For Logistic Regression: ')
print(best_params)
print("-"*100)

# Train the model with the best hyperparameters
best_model_logreg.fit(X_train, y_train)

# Predict probabilities for the validation set
y_pred_prob = best_model_logreg.predict_proba(X_test)[:,1]
roc_auc_lr = roc_auc_score(y_test, y_pred_prob)

# Evaluate the model on the test data
y_pred = best_model_logreg.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
cr_lr = classification_report(y_test, y_pred)
accuracy_lr_best = accuracy_score(y_test, y_pred) * 100

# Print the evaluation matrix
print('Logistic Regression after Hyperparameter Tuning:')
print("Confusion Matrix: \n", cm)
print("Classification Report:\n", cr_lr)
print("Accuracy: ", accuracy_lr_best)
print('F1 Score:', f1)
print("ROC AUC Score:", roc_auc_lr)
print()

from sklearn.model_selection import learning_curve

# Define the logistic regression model (already trained with the best hyperparameters)
model_logreg = best_model_logreg

# Generate the learning curve data
train_sizes, train_scores, test_scores = learning_curve(model_logreg, X_train, y_train, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10))

# Calculate the mean and standard deviation of training and test scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, label='Training Accuracy')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.3)
plt.plot(train_sizes, test_mean, label='Validation Accuracy')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.3)
plt.title('Learning Curve - Logistic Regression')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

"""### K Nearest Neighbour"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score


# Create a KNN classifier with k=4
knn = KNeighborsClassifier(n_neighbors=4)

# Train the classifier using the training data
knn.fit(X_train, y_train)

# Make predictions on the test data
y_pred_knn = knn.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)*100
f1 = f1_score(y_test,y_pred_knn)

# Generate the confusion matrix
confusion_mat = confusion_matrix(y_test, y_pred_knn)
print("Confusion Matrix for :")
print(confusion_mat)

# Generate the classification report
classification_rep = classification_report(y_test, y_pred_knn)
print("Classification Report:")
print(classification_rep)

print("F1 score: ", f1)
print("Accuracy: ", accuracy_knn)

# Hyperparameter tuning for KNN
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import roc_auc_score


# Define the hyperparameter grid
param_grid = {
    'n_neighbors': [3, 5, 7, 9],          # Number of neighbors
    'weights': ['uniform', 'distance'],   # Weighting scheme
    'p': [1, 2],                          # Distance metric (1: Manhattan, 2: Euclidean)
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],   # Algorithm used for nearest neighbors search
    'leaf_size': [10, 20, 30],             # Leaf size for tree-based algorithms
    'metric': ['euclidean', 'manhattan']   # Distance metric for tree-based algorithms
}

# Perform grid search with cross-validation
grid_search = GridSearchCV(knn, param_grid, cv=3)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters and evaluate on test data
best_params = grid_search.best_params_
knn_best = KNeighborsClassifier(**best_params)
knn_best.fit(X_train, y_train)
y_pred_knn_best = knn_best.predict(X_test)
accuracy_knn_best = accuracy_score(y_test, y_pred_knn_best)*100
f1 = f1_score(y_test,y_pred_knn_best)

# Predict probabilities for the validation set
y_pred_prob = knn_best.predict_proba(X_test)[:,1]
roc_auc_rf = roc_auc_score(y_test,y_pred_prob)

print("Best hyperparameter for KNN: ", best_params)
print("-"*100)

print("Best KNN model after hyperparamter tuning:")
# Generate the confusion matrix for best model
confusion_mat_best = confusion_matrix(y_test, y_pred_knn_best)
print("Confusion Matrix (Best Model for KNN):")
print(confusion_mat_best)

# Generate the classification report for best model
classification_rep_best = classification_report(y_test, y_pred_knn_best)
print("Classification Report (Best Model for KNN):")
print(classification_rep_best)

print("Accuracy: " , accuracy_knn_best)
print("F1 score: ", f1)
print("ROC AUC score: ", roc_auc_rf)

from sklearn.model_selection import learning_curve

# Define the KNN model with the best hyperparameters
model_knn = knn_best

# Generate the learning curve data
train_sizes, train_scores, test_scores = learning_curve(model_knn, X_train, y_train, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10))

# Calculate the mean and standard deviation of training and test scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, label='Training Accuracy')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.3)
plt.plot(train_sizes, test_mean, label='Validation Accuracy')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.3)
plt.title('Learning Curve - KNN')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

"""##### Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Create an instance of the Gradient Boosting Classifier
model_gbc = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=1.0,
    random_state=42
)

# Train the model
model_gbc.fit(X_train, y_train)

# Make predictions
y_pred = model_gbc.predict(X_test)

# Model evaluation
scores_gbc = cross_val_score(model_gbc, X_train,y_train, cv=5)
cm_gbc = confusion_matrix(y_test,y_pred)
cr_gbc = classification_report(y_test, y_pred)
accuracy_gbc = accuracy_score(y_test,y_pred)*100
f1_gbc = f1_score(y_test,y_pred)

print('Confusion Matrix:\n',cm_gbc)
print('Classification Report for Gradient Boosting Classifier:')
print(cr_gbc)
print('\nCross-calidation scores: ', scores_gbc)
print('Mean score: ', scores_gbc.mean())
print('Standard deviation: ', scores_gbc.std())
print('F1 Score:', f1_gbc)
print('Accuracy: ', accuracy_gbc)

# Hyperparameter tuning for Gradient Boosting Classifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score

# Define the hyperparameter grid for tuning
param_grid = {
    'n_estimators': [100, 200, 300],
    'learning_rate': [0.1, 0.01],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0]
}

# Create a GridSearchCV object
grid_search_gbc = GridSearchCV(estimator=model_gbc, param_grid=param_grid, cv=5)

# Fit the GridSearchCV object to the training data
grid_search_gbc.fit(X_train, y_train)

# Get the best hyperparameters and the corresponding model
best_params = grid_search_gbc.best_params_
best_model_gbc = grid_search_gbc.best_estimator_

# Print the results
print('Best Hyperparameters for Gradient Boosting Classifier:', best_params)


# Train the best model with the optimized hyperparameters
best_model_gbc.fit(X_train, y_train)

# Make predictions using the best model
y_pred = best_model_gbc.predict(X_test)

# Calculate the ROC AUC score
y_pred_prob = best_model_gbc.predict_proba(X_test)[:, 1]
roc_auc_gbc = roc_auc_score(y_test, y_pred_prob)

# Evaluate the best model
cm_gbc_best = confusion_matrix(y_test, y_pred)
cr_gbc_best = classification_report(y_test, y_pred)
accuracy_gbc_best = accuracy_score(y_test, y_pred) * 100
f1_gbc_best = f1_score(y_test, y_pred)

print('Gradient Boosting Classifier After Hyperparameter Tuning:')

# Print the results
print('Confusion Matrix:\n', cm_gbc_best)
print('Classification Report for Gradient Boosting Classifier:')
print(cr_gbc_best)

#print the cross validation scores
print('Accuracy: ', accuracy_gbc_best)
print('F1 Score:', f1_gbc_best)
print('ROC AUC Score:', roc_auc_gbc)

from sklearn.model_selection import learning_curve

# Define the GBC model (already trained with the best hyperparameters)
model_gbc = best_model_gbc

# Generate the learning curve data
train_sizes, train_scores, test_scores = learning_curve(model_gbc, X_train, y_train, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10))

# Calculate the mean and standard deviation of training and test scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, label='Training Accuracy')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.3)
plt.plot(train_sizes, test_mean, label='Validation Accuracy')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.3)
plt.title('Learning Curve - Gradient Boosting Classifier')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

"""## Support Vector Machine"""

from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
import joblib

from sklearn import metrics
ML_Model = []
accuracy = []
f1_score = []
recall = []
precision = []

#function to call for storing the results
def storeResults(model, a,b,c,d):
    ML_Model.append(model)
    accuracy.append(round(a, 3))
    f1_score.append(round(b, 3))
    recall.append(round(c, 3))
    precision.append(round(d, 3))

from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report,f1_score

# Create an instance of the SVM classifier
model_svm = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)

# Train the model
model_svm.fit(X_train, y_train)

# Make predictions
y_pred = model_svm.predict(X_test)

# Model evaluation
scores_svm = cross_val_score(model_svm, X_train, y_train, cv=5)
cm_svm = confusion_matrix(y_test, y_pred)
cr_svm = classification_report(y_test, y_pred)
accuracy_svm = accuracy_score(y_test, y_pred) * 100
f1_svm = f1_score(y_test, y_pred)

print('Confusion Matrix:\n', cm_svm)
print('Classification Report for SVM Classifier:')
print(cr_svm)
print('\nCross-validation scores: ', scores_svm)
print('Mean score: ', scores_svm.mean())
print('Standard deviation: ', scores_svm.std())
print('F1 Score:', f1_svm)
print('Accuracy: ', accuracy_svm)

from sklearn.model_selection import GridSearchCV

# Define the parameter grid
param_grid = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'gamma': ['scale', 'auto']
}

# Create an instance of the SVM classifier
model_svm = SVC(random_state=42)

# Create Grid Search object
grid_search = GridSearchCV(estimator=model_svm, param_grid=param_grid, cv=5, scoring='accuracy')

# Perform Grid Search to find the best hyperparameters
grid_search.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params = grid_search.best_params_
best_model_svm = grid_search.best_estimator_

# Print the best hyperparameters
print("Best Hyperparameters for SVM:", best_params)

# Make predictions using the best model
y_pred = best_model_svm.predict(X_test)

# Calculate the ROC AUC score
roc_auc_svm= roc_auc_score(y_test, y_pred)

# Model evaluation
cm_svm = confusion_matrix(y_test, y_pred)
cr_svm = classification_report(y_test, y_pred)
accuracy_svm_best = accuracy_score(y_test, y_pred) * 100
f1_svm = f1_score(y_test, y_pred)

print('Confusion Matrix:\n', cm_svm)
print('Classification Report for SVM Classifier:')
print(cr_svm)
print('Accuracy: ', accuracy_svm_best)
print('F1 Score:', f1_svm)
print('ROC AUC: ',roc_auc_svm)

from sklearn.model_selection import learning_curve

# Define the SVM model with the best hyperparameters
model_svm = best_model_svm

# Generate the learning curve data
train_sizes, train_scores, test_scores = learning_curve(model_svm, X_train, y_train, cv=5, scoring='accuracy', train_sizes=np.linspace(0.1, 1.0, 10))

# Calculate the mean and standard deviation of training and test scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)

# Plot the learning curve
plt.figure(figsize=(8, 6))
plt.plot(train_sizes, train_mean, label='Training Accuracy')
plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.3)
plt.plot(train_sizes, test_mean, label='Validation Accuracy')
plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.3)
plt.title('Learning Curve - SVM')
plt.xlabel('Training Set Size')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

"""# Model Selection"""

# Compare the model with accuracy
best_accuracy = [accuracy_lr_best, accuracy_rf_best, accuracy_gbc_best, accuracy_knn_best, accuracy_svm_best]

highest_value = best_accuracy[0]

# Loop through the array and update the highest value
for num in best_accuracy:
    if num > highest_value:
        highest_value = num

# Format the highest value with two digits and no decimal places
highest_value_formatted = "{:.0f}".format(highest_value)

# Print the highest value
if highest_value == accuracy_lr_best:
    print("Logistic Regression is the best model with the accuracy of", highest_value_formatted)
elif highest_value == accuracy_gbc_best:
    print("Gradient Boosting Classifier is the best model with the accuracy of", highest_value_formatted)
elif highest_value == accuracy_knn_best:
    print("K Nearest Neighbour is the best model with the accuracy of", highest_value_formatted)
elif highest_value == accuracy_rf_best:
    print("Random Forest is the best model with the accuracy of", highest_value_formatted)
elif highest_value == accuracy_svm_best:
    print("Support Vector Machine is the best model with the accuracy of {:.2f}".format(highest_value))

"""### Best Model After Hyperparameter Tuning is Logistic Regression"""

# Use the trained logistic regression model to predict with the testing dataset

# Remove the 'id' column from the test dataset
test_features = test.drop('id', axis=1)
train_features = train.drop('target', axis = 1)
train_scaled = scaler.fit_transform(train_features)
test_scaled = scaler.transform(test_features)

# Use the trained logistic regression model to predict with the test dataset features
predictions = best_model_logreg.predict(test_scaled)

# print(predictions)

# Create a DataFrame to store the predictions
results_lr = pd.DataFrame({'id': test['id'], 'target': predictions})

# Print the predictions
with pd.option_context('display.max_rows', None):
    print(results_lr)

"""##### Building Dashboard (REAL)"""

import ipywidgets as widgets
from IPython.display import display,clear_output
from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np
import warnings

warnings.filterwarnings('ignore')

# Create float input text boxes
gravity = widgets.FloatText(description='Gravity:')
ph = widgets.FloatText(description='pH value:')
osmo = widgets.FloatText(description='Osmo value:')
cond = widgets.FloatText(description='Conductivity:')
urea = widgets.FloatText(description='Urea concentration:',layout=widgets.Layout(width='70'), style={'description_width': 'initial'})
calc = widgets.FloatText(description='Calcium Concentration:',layout=widgets.Layout(width='70'), style={'description_width': 'initial'})

# Create button widget
button = widgets.Button(description= 'Make Prediction')
output_message = widgets.Output()

# Fuction to make prediction and print result

def make_prediction(button):
    with output_message:
        #Handle missong values
        gravity_value = gravity.value if gravity.value else np.nan
        ph_value = ph.value if ph.value else np.nan
        osmo_value = osmo.value if osmo.value else np.nan
        cond_value = cond.value if cond.value else np.nan
        urea_value = urea.value if urea.value else np.nan
        calc_value = calc.value if calc.value else np.nan

        # Check if any of the input values are missing
        if any(np.isnan([gravity_value,ph_value,osmo_value,cond_value,urea_value,calc_value])):
            print("Please provide values for all input fields.")
            return

        # # Create a user input array
        user_input = np.array([[gravity_value,ph_value,osmo_value,cond_value,urea_value,calc_value]],dtype=float)


        # transform the user input using the scaler:
        scaled_input = scaler.transform(user_input)
        # print(scaled_input)

        #Make predictions using the best model
        predictions_best = best_model_logreg.predict(scaled_input)
        print(predictions_best)

        #Process the predictions and return the result
        if predictions_best[0] == 1:
            results = "Kidney Stone Presence!"
        elif predictions_best[0] == 0:
            results = "Kidney Stone is not Presence!"
        else:
            results = "Invalid prediction"

        clear_output()

        print(results)


# Attach event listener to the buttun
button.on_click(make_prediction)

# Display the widges
display(gravity)
display(ph)
display(osmo)
display(cond)
display(urea)
display(calc)
display(button)
display(output_message)